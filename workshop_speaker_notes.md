# Speaker Notes - AI for Human Rights Experts

Total time: 45 minutes  
Deck: `/Users/lszoszk/Desktop/HURIDOCS/AI presentation/workshop_ai_rule_of_law_45min.html`

## Slide-level plan

| Slide | Target | Core objective |
|---|---:|---|
| 1 | 2 min | Set framing, outcomes, and baseline audience pulse |
| 2 | 1 min | Commit to strict timeboxes |
| 3 | 4 min | Show acceleration from historical diffusion to AI-era adoption |
| 4 | 3 min | Explain jagged frontier and workflow implications |
| 5 | 3 min | Cybernetic teammate evidence and coding narrative |
| 6 | 3 min | Coding as task; humans own problem-solving |
| 7 | 3 min | Benchmark trend interpretation |
| 8 | 3 min | Human-rights use-case matrix |
| 9 | 3 min | Hands-on readiness checks |
| 10 | 5 min | Prompt copy and execution instructions |
| 11 | 4 min | PDF attachment and run sequence |
| 12 | 4 min | Rubric scoring and risk checks |
| 13 | 2 min | Benefits/limitations and controls |
| 14 | 3 min | Expansion path and resource links |
| 15 | 2 min | Close and commitments |

## Minute-by-minute facilitation script

### 0:00-0:03 (Slides 1-2)
- Open with one sentence: "This is a practical workshop on what to deploy, what not to deploy, and how to keep human-rights accountability intact."
- Run a fast baseline poll on Slide 1 (show of hands or click options).
- State explicit outputs participants will leave with: prompt, sample document workflow, rubric, safeguards.
- On Slide 2, show exact timing blocks and commit to strict transitions.

### 0:03-0:10 (Slides 3-4)
- Slide 3: state the exact adoption comparison first (PC 15 years, Internet 7-8 years, Facebook 4.5 years, ChatGPT 2 months for 100M users).
- Use the alarm clock + window-knocker visuals first and emphasize the 1790s-to-1920s long diffusion period.
- Emphasize that scale and access inequality coexist.
- Bridge to human-rights relevance: information quality, public trust, and procedural fairness.
- Slide 4: explain "jagged frontier" in operational terms.
- Ask one quick question: "Where would a single model failure create human-rights harm in your workflow?"

### 0:10-0:16 (Slides 5-6)
- Slide 5: summarize NBER evidence (776 professionals) and explicitly connect it to AI as a cybernetic teammate.
- Replace pair work with chat-based participation: everyone posts one problem statement in chat.
- Slide 6: play short clip.
- Ask participants to post 3 lines in chat: problem, sample dataset idea, success metric.

### 0:16-0:22 (Slides 7-8)
- Slide 7: show benchmark chart first in p50 horizon mode.
- Optional: switch to average score mode to show metric framing differences.
- If YAML auto-load fails, explain file:// browser restrictions and point to fallback.
- Slide 8: map participants' tasks into the four use-case buckets.

### 0:22-0:36 (Slides 9-11)
- Slide 9: assign platform paths explicitly: Gemini + Canvas (recommended), Claude + Artifacts, or ChatGPT.
- Slide 10: insist on exact prompt, first run without edits, regardless of platform.
- Set timer: 10 minutes for execution.
- During execution, circulate and solve setup blockers only.
- Slide 11: ensure everyone uses the same sample PDF for comparability.

### 0:36-0:40 (Slide 12)
- Ask participants to score outputs before discussing improvements.
- Collect score ranges for each criterion.
- Use risk checks (hallucination/misleading legal framing) to structure debrief.
- Record one common error pattern and one common success pattern.

### 0:40-0:43 (Slides 13-13)
- Slide 13: summarize benefits and limits in one minute each.
- Add minimum control set: data classification, review gates, source links, logs.

### 0:43-0:45 (Slides 14-15)
- Slide 14: explain incremental path from prototype to controlled deployment and point to resources, including the General Comments search engine.
- Slide 15: collect one-week commitment from each participant.
- Share contact and follow-up channel.

## Prompt debrief questions
- Which part of the output looked most convincing but required the most correction?
- Which rubric criterion was weakest across participants?
- What one prompt tweak improved quality without increasing risk?

## Facilitator fallback plan (if tech issues occur)
- If participants cannot run models live: run one facilitator demo and switch to group rubric scoring.
- If internet is unstable: continue using local assets and prior generated outputs.
- If time slips by >2 minutes: shorten Slide 14 and move directly to commitments.

## Exit criteria
- Participants leave with a tested workflow pattern, not just conceptual awareness.
- Participants can name at least one safeguard they will apply in real work.
